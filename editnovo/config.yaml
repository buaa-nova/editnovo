###
# Editnovo configuration.
# Blank entries are interpreted as "None".
###

###
# The following parameters can be modified when running inference or when
# fine-tuning an existing Editnovo model.
###

# Max absolute difference allowed with respect to observed precursor m/z.
# Predictions outside the tolerance range are assigned a negative peptide score.
precursor_mass_tol: 50  # ppm
# Isotopes to consider when comparing predicted and observed precursor m/z's.
isotope_error_range: [0, 1]
# The minimum length of predicted peptides.
min_peptide_len: 6
# Number of spectra in one inference batch.
predict_batch_size: 32
# Number of beams used in beam search.
n_beams: 5
# Number of PSMs for each spectrum.
top_match: 1
# The hardware accelerator to use. Must be one of:
# "cpu", "gpu", "tpu", "ipu", "hpu", "mps", or "auto".
accelerator: "auto"
# The devices to use. Can be set to a positive number int, or the value -1 to
# indicate all available devices should be used. If left empty, the appropriate
# number will be automatically selected for based on the chosen accelerator.
devices:
###
# The following parameters should only be modified if you are training a new
# Editnovo model from scratch.
###

# Random seed to ensure reproducible results.
random_seed: 454

# OUTPUT OPTIONS
# Logging frequency in training steps.
n_log: 1
# Tensorboard directory to use for keeping track of training metrics.
tb_summarywriter: 
# Save the top k model checkpoints during training. -1 saves all, and leaving
# this field empty saves none.
save_top_k: 5
# Path to saved checkpoints.
model_save_folder_path: "/root/attennovo/checkpoint"
# Model validation and checkpointing frequency in training steps.
# val_check_interval: 5470
# val_check_interval: 50_000
# val_check_interval: 7_000
val_check_interval: 3_500
# val_check_interval: 1
hdf5_dir: 
dump_hdf5: False
n_workers: 8

# MODEL LOADING OPTIONS
# Set to true to force loading model from checkpoint file instead of using trainer.fit()
# This is useful when you want to update learning rate or other training parameters
force_load_from_checkpoint: True

# SPECTRUM PROCESSING OPTIONS
# Number of the most intense peaks to retain, any remaining peaks are discarded.
n_peaks: 150
# Min peak m/z allowed, peaks with smaller m/z are discarded.
min_mz: 50.0
# Max peak m/z allowed, peaks with larger m/z are discarded.
max_mz: 2500.0
# Min peak intensity allowed, less intense peaks are discarded.
min_intensity: 0.01
# Max absolute m/z difference allowed when removing the precursor peak.
remove_precursor_tol: 2.0  # Da
# Max precursor charge allowed, spectra with larger charge are skipped.
max_charge: 10

n_workers: 8  # Number of workers for data loading, set to 0 to disable multiprocessing.

# MODEL ARCHITECTURE OPTIONS
# Dimensionality of latent representations, i.e. peak embeddings.
dim_model: 512
# Number of attention heads.
n_head: 8
# Dimensionality of fully connected layers.
dim_feedforward: 1024
# Number of transformer layers in spectrum encoder and peptide decoder.
n_layers: 9
# Number of decoder iterations
max_decoder_iters: 9
# Dropout rate for model weights.
dropout: 0.1
# Number of dimensions to use for encoding peak intensity.
# Projected up to `dim_model` by default and summed with the peak m/z encoding.
dim_intensity:
# Max decoded peptide length.
max_length: 100 #20_000
# The number of iterations for the linear warm-up of the learning rate.
# warmup_iters: 100_000 #500_000
# # warmup_iters: 10940
# # The number of iterations for the cosine half period of the learning rate.
# cosine_schedule_period_iters: 600_000 #600_000
# # Learning rate for weight updates during training.
# learning_rate: 5e-4
# warmup_iters: 27_000 #5*1820, 14500
warmup_iters: 11_000
# warmup_iters: 100_000 #5*1820
# warmup_iters: 10940
# The number of iterations for the cosine half period of the learning rate.
cosine_schedule_period_iters: 250_000 
# cosine_schedule_period_iters: 600_000 #35*1820, 600_000
# The number of iterations to keep constant learning rate in the constant phase scheduler.
constant_lr_iters: 100 # 10_000
# The number of iterations for final decay from constant LR to minimum LR.
final_decay_iters: 20_000 # 137_000
# Learning rate for weight updates during training.
learning_rate: 3e-4 #5e-4
# learning_rate: 5e-5 #2e-5

# Regularization term for weight updates.
weight_decay: 1e-5
# Minimum learning rate factor as a fraction of the base learning rate.
min_lr_factor: 6e-5
# Amount of label smoothing when computing the training loss.
train_label_smoothing: 0.01

# TRAINING/INFERENCE OPTIONS
# Number of spectra in one training batch.
train_batch_size: 1024 #512
# Max number of training epochs.
max_epochs: 11 # 15 #30
# Number of validation steps to run before training begins.
num_sanity_val_steps: 0
# Calculate peptide and amino acid precision during training.
# This is expensive, so we recommend against it.
calculate_precision: True

# for training
dual_training_for_deletion: False
dual_training_for_insertion: False
no_share_discriminator: False
sampling_model_gen: 0.6

# for inference
allow_sampling: False
# placeholder sampling number 
top_k_for_mask_insert: 5
# tokendecoder top_k
top_k_for_word_insert: 10
# tokendecoder sampling number
sampling_num: 100

tmp_dir: 

# AMINO ACID AND MODIFICATION VOCABULARY
residues:
  "G": 57.021464
  "A": 71.037114
  "S": 87.032028
  "P": 97.052764
  "V": 99.068414
  "T": 101.047670
  "C+57.021": 160.030649 # 103.009185 + 57.021464
  "L": 113.084064
  "I": 113.084064
  "N": 114.042927
  "D": 115.026943
  "Q": 128.058578
  "K": 128.094963
  "E": 129.042593
  "M": 131.040485
  "H": 137.058912
  "F": 147.068414
  "R": 156.101111
  "Y": 163.063329
  "W": 186.079313
  # Amino acid modifications.
  "M+15.995": 147.035400    # Met oxidation:   131.040485 + 15.994915
  "N+0.984": 115.026943     # Asn deamidation: 114.042927 +  0.984016
  "Q+0.984": 129.042594     # Gln deamidation: 128.058578 +  0.984016
  # N-terminal modifications.
  "+42.011": 42.010565      # Acetylation
  "+43.006": 43.005814      # Carbamylation
  "-17.027": -17.026549     # NH3 loss
  "+43.006-17.027": 25.980265      # Carbamylation and NH3 loss
